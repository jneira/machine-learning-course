* Cost function
  - Training set: {(x[1],y[1]),(x[2],y[2]),...,(x[m],y[m])}
  - L = total no. of layers in network
  - S[l]=nº of units (not counting bias unit) in layer l
  - Binary classification (y=0 or 1) -> 1 output unit
  - Multiclass classification (K clases):
    - y ∈ R^K
    - K output units
  - Cost function is a generalization of the logistic one: K logisitc
    output instead of one
  - h[theta](x) ∈ R^K  (h[theta](x))[i]=ith output
  - J(theta)=-1/m [∑(i=1..m)∑(k=1..K) y[k,i] log(h[theta](x[i]))[k] +
    (1-y[k,i]) log(1-(h[theta](x[i]))[k])] + λ/2m
    ∑(l=1..L-1)∑(i=1..S[l])∑(j=1..S[l+1] (theta[l,j,i)^2
* Backpropagation algorithm
  - min J(theta)
  - compute J(theta), d/d theta[i,j,l] J(theta)
  - delta[j,l]="error" of node j in layer l
  - For each output unit (layer L=4) delta[j,4]=a[j,4]-y[j] 
  - a[j,4]=(h[theta][x])[j]
  - delta[3]=((theta[3])^T delta[4]).*g'(z[3])=a[3].*(1-a[3])
  - delta[2]=((theta[2])^T delta[3]).*g'(z[2])=a[2].*(1-a[2])
  - d/d(theta[i,j,l]) J(theta)=a[j,l]delta[i,l+1]
  - Δ[i,j,l]=0 (for all l,i,j)
  - For i=1 to m
    - Set a[1]=x[i]
    - Perform forward propagation to compute a[l] for l=2,3,...,L
    - Using y[i],compute delta[L]=a[L]-y[i]
    - Compute delta[L-1],delta[L-2],...,delta[2] (no delta[1])
    - Δ[i,j,l]:=Δ[i,j,l]+a[j,l]delta[i,l+1]
    - Δ[l]:=Δ[l]+delta[l+1] (a[l])^T

* Backpropagation intuition
  
